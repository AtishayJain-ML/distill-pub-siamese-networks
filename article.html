<meta charset="utf-8">


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script src="https://distill.pub/template.v1.js"></script>

<script>

$(document).ready(function () { // this need jquery
  renderMathInElement(document.body, {
    // ...options...
    delimiters: [
      { left: "$$", right: "$$", display: true },
      { left: "$", right: "$", display: false },
      { left: "\\[", right: "\\]", display: true }
    ]
  });
});
</script>


<script>

function show_image(val) {
  document.getElementById("demo").innerHTML = "Margin = " + val;
  document.getElementById("caption1").innerHTML = "<strong>Figure 9 </strong>- Train data embeddings after training using triplet loss with margin " + val;
  document.getElementById("caption2").innerHTML = "<strong>Figure 10 </strong>- Test data embeddings after training using triplet loss with margin " + val;
  
    var img = document.createElement("img");
    var img2 = document.createElement("img");
    img.src = "margin_viz/train_margin_"+val+"_axis_40.png";
    img2.src = "margin_viz/val_margin_"+val+"_axis_40.png";
    document.canvas.src = img.src
    document.canvas2.src = img2.src
    img.width = 100;
    img.height = 100;
    img.alt = 'Margin Visualization';
}


function show_image_epoch(val) {
  document.getElementById("epoch_demo").innerHTML = "Epoch Number = " + val;
  document.getElementById("caption3").innerHTML = "<strong>Figure 7 </strong>- Train data embeddings after training for " + val + " epochs";
  document.getElementById("caption4").innerHTML = "<strong>Figure 8 </strong>- Test data embeddings after training for " + val + " epochs";
  
    var img = document.createElement("img");
    var img2 = document.createElement("img");
    img.src = "epoch_wise_viz/new_margin_10_train_epoch_" + val + ".png";
    img2.src = "epoch_wise_viz/new_margin_10__epoch_val"+val+".png";
    document.canvas3.src = img.src
    document.canvas4.src = img2.src
    img.width = 100;
    img.height = 100;
    img.alt = 'Epoch Visualization';
}


function show_image_1112(val) {
  document.getElementById("demo-11-12").innerHTML = "Epoch Number = " + val;
  document.getElementById("caption11").innerHTML = "<strong>Figure 11 </strong>- Train data embeddings after training for " + val + " epochs with L2 distance metric";
  document.getElementById("caption12").innerHTML = "<strong>Figure 12 </strong>- Test data embeddings after training for " + val + " epochs with L2 distance metric";
  
    var img = document.createElement("img");
    var img2 = document.createElement("img");
    img.src = "distance_viz/L2_train_epoch_" + val + ".png";
    img2.src = "distance_viz/L2_test_epoch_"+ val +".png";
    document.canvas11.src = img.src
    document.canvas12.src = img2.src
    img.width = 100;
    img.height = 100;
    img.alt = 'Epoch Visualization';
}


function show_image_1314(val) {
  document.getElementById("demo-13-14").innerHTML = "Epoch Number = " + val;
  document.getElementById("caption13").innerHTML = "<strong>Figure 13 </strong>- Train data embeddings after training for " + val + " epochs with L1 distance metric";
  document.getElementById("caption14").innerHTML = "<strong>Figure 14 </strong>- Test data embeddings after training for " + val + " epochs with L1 distance metric";
  
    var img = document.createElement("img");
    var img2 = document.createElement("img");
    img.src = "distance_viz/L1_train_epoch_" + val + ".png";
    img2.src = "distance_viz/L1_test_epoch_"+val+".png";
    document.canvas13.src = img.src
    document.canvas14.src = img2.src
    img.width = 100;
    img.height = 100;
    img.alt = 'Epoch Visualization';
}

</script>


<script type="text/front-matter">
  title: "Siamese Networks and Triplet Loss"
  description: "Exploring siamese networks and different strategies of training them using triplet loss"
  authors:
  - Atishay Jain*
  - Varun Gohil*
  - S. Deepak Narayanan*
  affiliations:
  - IIT Gandhinagar
  - IIT Gandhinagar
  - IIT Gandhinagar
</script>
<body>
  <dt-fn>* Denotes equal contrbution. Authorship order decided by rolling a die</dt-fn>
<dt-article>

  <h1>Siamese Networks and Triplet Loss</h1>
  <h2>Exploring siamese networks and different strategies of training them using triplet loss</h2>
  <dt-byline></dt-byline>


  <p>Deep neural networks have shown to give highly accurate results across problems in machine learning. However, this accuracy comes at a price which manifests in the form of high data requirements. To alleviate the problem of low amounts of data, techniques such as data augmentation, transfer learning and one-shot learning are used</p>
  <p>In this article we introduce and explain one-shot learning using siamese networks and triplet loss. We will look at what siamese networks are and what kind of loss functions are used. We will examine the advantages of siamese networks over state-of-the-art classifiers with the example of MNIST data. We will then further explore triplet loss and how to select ‘useful’ triplets for training siamese networks. Continuing with the example of MNIST, we shall examine the advantages of triplet loss and the different triplet selection methods.</p>

  <h2>One Shot Learning</h2>
  <p>One shot learning is a classification technique used to train neural networks using only one or a few training samples. Apart from the advantage of requiring fewer samples than usual classification algorithms, it allows for increasing the number of classes without having to change the architecture of the model.</p>
  <p>In one shot learning, we aim to create a distance metric or a similarity function which brings samples of the same class closer to each other and those of different classes farther from each other. Therefore, we can judge which class a test sample belongs to be measuring the distance of the sample from the different classes. </p>

  <h2>Siamese Networks</h2>
  <p>Siamese networks can be used for learning the distance metric for one-shot learning. In a siamese network, we try to create embeddings of the samples such that the distance between samples of the same class is lower and between those of different classes is higher. A siamese network consists of two exact same neural networks with identical weights. We then pass a sample each into both the networks, calculate the loss and then depending on whether they are from the same class or different classes, we update the weights of both the networks. Since both the networks are identical, their weights will still remain the same after backpropagation. </p>

  <p>In siamese networks,  a loss is calculated between the embeddings of both the networks. This automatically triggers the question as to what loss can be used for such networks. Two popular losses used in siamese networks are:
  <ol>
    <li>Contrastive Loss </li>
    <li>Triplet Loss</li>
  </ol>
  </p>

  <h2>Contrastive Loss</h2>

  <p>The contrastive loss <dt-cite key="contrastive"></dt-cite> is defined between embeddings of two different inputs as :
  \[Contrastive \, Loss = \frac{1}{2} \times (1-w) \times d(x,y)^2 + \frac{1}{2} \times w \times max(0, m - d(x,y))^2\]
  where, <ul style="list-style: none; margin-top: -25"> 
    <li style="margin: 0;padding: 0em;">x = input sample 1</li>
    <li style="margin: 0;padding: 0em;">y = input sample 2</li>
    <li style="margin: 0;padding: 0em;">d(x,y) = distance metric between the embeddings of x and y</li> 
    <li style="margin: 0;padding: 0em;">w = binary variable which tells us whether x and y are in the same class (w = 0) or different classes (w = 1)</li> 
    <li style="margin: 0;padding: 0em;">m = margin</li>
</ul>
  </p>

  <p>  Essentially, siamese networks try to create an embedding of the inputs such that inputs of the same class cluster together. The margin ‘m’ decides the distance between different clusters. Furthermore, the distance function can be any distance metric (for example euclidean distance).</p>
<p>

  <style>
  #compare-cw {
    display: grid;
    grid-gap: 40px;
    grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
  }
  #compare-cw .subfigure {
      display: grid;
      grid-gap: 20px;
      grid-template-columns: repeat(2, minmax(160px, 1fr));
      /* grid-auto-rows: min-content; */
      /* grid-template-rows: min-content auto; */
    }
  

  #compare-cw .figcaption {
    text-align: center;
    padding-top: 5px;
    margin-top: 5px;
      /* min-height: 179px; */
      /* min-width: 179px; */
      /* flex: 1; */
  }


  </style>

<figure id="compare-cw" class="l-page">
      <div class="subfigure">
        <h5 style="grid-column: span 2;" class="column-heading">Cons of Contrastive Contrastive Loss</h5>
        <div style="grid-column: 1;" class="evidence">
            <img src="static_images/image2.png" alt="CWtrain">
            <div class="figcaption">
                <strong>Figure 1 </strong>- Train data embeddings after training using contrastive loss
            </div>
        </div>
        <div style="grid-column: 2;" class="estimate">
            <img src="static_images/image4.png" alt="CWval">
            <div class="figcaption">
                <strong>Figure 2 </strong>- Test data embeddings after training using contrastive loss
            </div>
        </div>
      </div>
      
  </figure>

</p>
  <p>Figure 1 and Figure 2 show the 2-dimensional embeddings learned by training siamese networks with contrastive loss for 30 epochs. As you can see in Figure 1 above, using contrastive loss the embeddings of training inputs of the same class converge very close to each other. However, we see that is not the case for the test samples. We can see in Figure 2, that the embeddings do not cluster properly and that there are many overlaps. This hints to an evidence of overfitting when we use contrastive loss. Hence we use another type of loss - triplet loss.</p>

  <h2>Triplet Loss</h2>


  <p>As seen earlier, Contrastive loss does not generalize well as it forces samples belonging to the same class to collapse to a single point. Triple loss ameliorates this problem by enforcing the following constraint: <br/>
    Embeddings of samples belonging to the same class must be closer to each other than embeddings of samples belonging to different classes.
</p>

  <p>Triplet Loss is defined over triplets of samples in the dataset. Every triplet has 3 samples, each of which falls into a distinct category out of the ones given below:
    <ol>
      <li>Anchor: Can be any sample of the dataset. The remaining two categories are defined on the basis of the anchor’s class</li>
      <li>Positive: A sample from the dataset which belongs to the same class as the anchor</li>
      <li>Negative: A sample from the dataset which does not belong to the same class as the anchor</li>
    </ol>

</p>
  <p>
    Formally, the triplet loss can be defined as:

\[Triplet \, loss = max( d(a,p) - d(a,n) + m, 0)\]

where, <ul style="list-style: none; margin-top: -25">
    <li style="margin: 0;padding: 0em;">a =  anchor sample</li>
    <li style="margin: 0;padding: 0em;">p = positive sample</li>
    <li style="margin: 0;padding: 0em;">n = negative sample </li> 
    <li style="margin: 0;padding: 0em;">d(x,y) = distance metric between the embeddings of x and y</li> 
    <li style="margin: 0;padding: 0em;">m = margin</li>
</ul>


  </p>

<p>Just like contrastive loss, m is a hyperparameter which dictates the distance between the clusters of two different classes.
</p>


<p>
<figure id="compare-cw" >
      <div class="subfigure" style ="grid-template-columns: repeat(1, minmax(160px, 1fr));">
        
        <div style="grid-column: 1;" class="evidence">
            <img src="static_images/image1.png" alt="TLTriplets">
            <div class="figcaption">
                <strong>Figure 3 </strong> - How triplet loss learns embeddings
            </div>
        </div>
       
      </div>
      
  </figure>
</p>
<p>Let us consider the MNIST dataset again. While training, our aim is to minimize the triplet loss. Hence, the d(a,p) term which signifies the distance between the embeddings of two samples of the same class is pushed towards zero. Therefore, if the anchor is of class ‘5’, another image of the class ’5’ would be chosen and distance between their embeddings would be reduced. For the negative sample, an image belonging to a class other than ‘5’ would be selected. Let us assume the class of the negative image is ‘6’. Since there is a negative sign in front of the d(a,n) term, minimizing the loss would increase this term. Therefore the distance between the embeddings of the ‘5’ class anchor and ‘6’ class negative sample will increase. Furthermore, to bring the loss to 0, d(a,n) must be increased until it is greater than the sum of the margin and d(a,p).</p>

<p>
<figure id="compare-cw" >
      <div class="subfigure" style ="grid-template-columns: repeat(1, minmax(160px, 1fr));">
        
        <div style="grid-column: 1;" class="evidence">
            <img src="static_images/mnist_triplet.png" alt="TLNetwork">
            <div class="figcaption">
                <strong>Figure 4 </strong>- Network for training with triplet loss
            </div>
        </div>
       
      </div>
      
  </figure>
</p>
<p>For training, the anchor, positive sample and the negative sample are each given as an input to three networks that are identical in structure and weights. These networks output the embedding of these 3 samples and these embeddings are used to compute the triplet loss. The loss is accumulated over all triplets formed by a batch of N samples and after the loss has been computed for all triplets, the gradients are back-propagated. </p>

<p>As triplet loss just aims to have a positive sample closer to anchor than the negative sample, it does not force all samples of the same class to collapse to a single point. This is illustrated in the Figures 5 and 6 below. As can be seen, it performs much better on the test set and also generalises well compared to contrastive loss.</p>

<figure id="compare-cw" class="l-page">
      <div class="subfigure">
        <!-- <h5 style="grid-column: span 2;" class="column-heading">Triplet Loss Comparison</h5> -->
        <div style="grid-column: 1;" class="evidence">
            <img src="static_images/train_epoch30.png" alt="CWtrain">
            <div class="figcaption">
                <strong>Figure 5 </strong>- Train data embeddings after training using triplet loss
            </div>
        </div>
        <div style="grid-column: 2;" class="estimate">
            <img src="static_images/test_epoch30.png" alt="CWval">
            <div class="figcaption">
                <strong>Figure 6 </strong>- Test data embeddings after training using triplet loss
            </div>
        </div>
      </div>
      
  </figure>

<p>The embeddings of the samples after each epoch for both train and test data can be seen in the visualization below. We can see that in the first few epochs the clusters become smaller. This can be attributed to the fact that triplet loss minimizes the distance between samples of same class. We can also see that the distance between clusters of different classes increase during the initial epochs. This is because triplet loss forces the samples from opposite classes to be farther away from each other than samples from same class. Finally we observe that after 20 epochs the clusters do not change significantly.</p>

<figure id="compare-cw" class="l-page">
      <div class="subfigure">
        <!-- <h5 style="grid-column: span 2;" class="column-heading">Contrastive Loss Comparison</h5> -->
        <div style="grid-column: 1;" class="evidence">
  <img src="epoch_wise_viz/new_margin_10_train_epoch_1.png" name="canvas3" style="height: 500px;width: 500px" />
            <div class="figcaption" id="caption3">
                <strong>Figure 7 </strong>- Train data embeddings after training for 1 epoch
            </div>
<!--   <input type="range" name="weight" align="center" id="range_weight" value="1" min="1" max="20" oninput="show_image(range_weight.value)">
  <p id="demo">Margin = 1</p> -->
        </div>
        <div style="grid-column: 2;" class="estimate">
  <img src="epoch_wise_viz/new_margin_10__epoch_val1.png" name="canvas4" style="height: 500px;width: 500px" />
            <div class="figcaption" id="caption4">
                <strong>Figure 8 </strong>- Test data embeddings after training for 1 epoch 
            </div>
        </div>
      </div>
      
  </figure>
  <center>
      <input type="range" name="weight" align="center" id="range_weight_epoch" value="1" min="1" max="30" oninput="show_image_epoch(range_weight_epoch.value)">
  <br>
    <p id="epoch_demo" align="center">Epoch Number = 1</p>
    
  </center>

<p>The triplet loss has only one hyperparameter - the margin. It represents the distances between two clusters of samples of different classes. The greater the margin, the greater the inter-cluster distance. The effect of changing the margin can be visualized by changing the margin parameter and seeing the output in Figure 9 and 10. For this visualization we train siamese networks with triplet loss for 30 epochs for values of margin ranging from 1 to 20.</p>

<figure id="compare-cw" class="l-page">
      <div class="subfigure">
        <!-- <h5 style="grid-column: span 2;" class="column-heading">Contrastive Loss Comparison</h5> -->
        <div style="grid-column: 1;" class="evidence">
  <img src="margin_viz/train_margin_1_axis_40.png" name="canvas" style="height: 500px;width: 500px" />
            <div class="figcaption" id="caption1">
                <strong>Figure 9 </strong>- Train data embeddings after training using triplet loss with margin 1
            </div>
<!--   <input type="range" name="weight" align="center" id="range_weight" value="1" min="1" max="20" oninput="show_image(range_weight.value)">
  <p id="demo">Margin = 1</p> -->
        </div>
        <div style="grid-column: 2;" class="estimate">
  <img src="margin_viz/train_margin_1_axis_40.png" name="canvas2" style="height: 500px;width: 500px" />
            <div class="figcaption" id="caption2">
                <strong>Figure 10 </strong>- Test data embeddings after training using triplet loss with margin 1
            </div>
        </div>
      </div>
      
  </figure>
  <center>
      <input type="range" name="weight" align="center" id="range_weight" value="1" min="1" max="20" oninput="show_image(range_weight.value)">
  <br>
    <p id="demo" align="center">Margin = 1</p>
    
  </center>

</p>

<!-- <p align = "center">
  <img src="margin_viz/train_margin_1_axis_40.png" name="canvas" style="height: 500px;width: 500px" />
<br>
<input type="range" name="weight" alilgn="center" id="range_weight" value="1" min="1" max="20" oninput="show_image(range_weight.value)">

</p> -->




<!-- <p id="demo" align="center">Margin = 1</p> -->
  
  <p>
    For the above visualizations, we have used the L2-squared distance as the distance metric. However, any distance metric can be used. The effect of using different distance functions on the learned embeddings can be seen using the visualization below. Figures 11 and 12 show the embeddings learned after each epoch using L2 distance metric. Figure 13 and 14 show the embeddings learned after each epoch for L1 distance metric. Using the L2 distance has an added advantage that the margin is more interpretable, as it represents the absolute distance. Overall, the distance metrics of L1, L2 and L2-squared all perform well.
  </p> 


<figure id="compare-cw" class="l-page">
      <div class="subfigure">
        <!-- <h5 style="grid-column: span 2;" class="column-heading">Contrastive Loss Comparison</h5> -->
        <div style="grid-column: 1;" class="evidence">
  <img src="distance_viz/L2_train_epoch_1.png" name="canvas11" style="height: 500px;width: 500px" />
            <div class="figcaption" id="caption11">
                <strong>Figure 11 </strong>- Train data embeddings after training for 1 epoch with L2 distance metric
            </div>
<!--   <input type="range" name="weight" align="center" id="range_weight" value="1" min="1" max="20" oninput="show_image(range_weight.value)">
  <p id="demo">Margin = 1</p> -->
        </div>
        <div style="grid-column: 2;" class="estimate">
  <img src="distance_viz/L2_test_epoch_1.png" name="canvas12" style="height: 500px;width: 500px" />
            <div class="figcaption" id="caption12">
                <strong>Figure 12 </strong>- Test data embeddings after training for 1 epoch with L2 distance metric
            </div>
        </div>
      </div>
      
  </figure>
  <center>
      <input type="range" name="weight" align="center" id="range_weight_1112" value="1" min="1" max="30" oninput="show_image_1112(range_weight_1112.value)">
  <br>
    <p id="demo-11-12" align="center">Epoch Number = 1</p>
    
  </center>


<figure id="compare-cw" class="l-page">
      <div class="subfigure">
        <!-- <h5 style="grid-column: span 2;" class="column-heading">Contrastive Loss Comparison</h5> -->
        <div style="grid-column: 1;" class="evidence">
  <img src="distance_viz/L1_train_epoch_1.png" name="canvas13" style="height: 500px;width: 500px" />
            <div class="figcaption" id="caption13">
                <strong>Figure 13 </strong>- Train data embeddings after training for 1 epoch with L1 distance metric
            </div>
<!--   <input type="range" name="weight" align="center" id="range_weight" value="1" min="1" max="20" oninput="show_image(range_weight.value)">
  <p id="demo">Margin = 1</p> -->
        </div>
        <div style="grid-column: 2;" class="estimate">
  <img src="distance_viz/L1_test_epoch_1.png" name="canvas14" style="height: 500px;width: 500px" />
            <div class="figcaption" id="caption14">
                <strong>Figure 14 </strong>- Test data embeddings after training for 1 epoch with L1 distance metric
            </div>
        </div>
      </div>
      
  </figure>
  <center>
      <input type="range" name="weight" align="center" id="range_1314" value="1" min="1" max="30" oninput="show_image_1314(range_1314.value)">
  <br>
    <p id="demo-13-14" align="center">Epoch Number = 1</p>
    
  </center>


  <p>Although triplet loss might seem to be a panacea, it does have its own cons. A major con of triplet loss is its scalability with respect to dataset size. As the size of datasets increase, the number of possible triplets also increases cubically. For example, for a batch of N samples there are O(N<sup>3</sup>) possible triplets. This makes training over all the triplets of large datasets impractical. </p>

  <p>This problem can be alleviated if one smartly chooses the triplets for training. Ideally one would like to select a triplet which has the anchor, the hardest positive and the hardest negative, where hardest positive = argmax<sub>p</sub>(d(a,p)) and hardest negative = argmin<sub>n</sub>(d(a,n)). However, for a large dataset, it might be infeasible to compute argmax and argmin. Further, selecting the hardest samples might also result in poor training due to sample mislabelling and presence of outliers. </p>

  <p>The very definition of triplet loss classifies each triplet in three different categories. This classification of triplets can be used to select triplets for training. The three categories are as follows:
    <ol>
      <li>Easy Triplets : Triplets which have a loss of zero i,e d(a,n) > d(a,p) + margin</li>
      <li>Semi-hard Triplets : Triplets where the negative is not closer to the anchor than the positive, but which still have positive loss i.e d(a, p) + margin > d(a,n) > d(a, p)</li>
      <li>Hard Triplets: Triplets for which positive is farther away from the anchor than the negative i.e. d(a, p) > d(a,n)</li>
    </ol>
</p>

<figure id="compare-cw" >
      <div class="subfigure" style ="grid-template-columns: repeat(1, minmax(160px, 1fr));">
        
        <div style="grid-column: 1;" class="evidence">
            <img src="static_images/image5.png" alt="TLTriplettypes">
            <div class="figcaption">
                <strong>Figure 15 </strong>-Types of triplets
            </div>
        </div>
       
      </div>
      
  </figure>
</p>

<p>The easy triplets already satisfy the constraint of triplet loss and do not contribute while training. Using easy triplets, leads to slower convergence and wasted computational resources. Hence one should avoid easy triplets while training. Selecting the hardest negatives can in practice lead to bad local minima early on in training, specifically, it can result in a collapsed model <dt-cite key="facenet"></dt-cite>. Hence for training, we avoid selecting easy or hard triplets and select semi-hard triplets. These semi-hard triplets generally lie inside the margin area as can be seen in Figure 15 above</p>

<h2>Triplet Mining</h2>
<p>Now that we know which triplets to select, we need to define a strategy to select these triplets before each epoch. The procedure for selecting useful triplets is known as triplet mining. It can be divided into two broad categories:
<ol>
  <li>Offline Triplet Mining: All triplets generated at beginning. Not a good method for selecting triplets due to the high overhead of generating all triplets once.</li>
  <li>Online Triplet Mining: All triplets generated while training. Within online triplet mining there are a few techniques.
    <ol type = "a">
      <li>Batch all: Select all the valid triplets, and average the loss on the hard and semi-hard triplets <dt-cite key="tripletdefense"></dt-cite>. 
        <ul>
          <li>A crucial point here is to not take into account the easy triplets (those with loss 0), as averaging on them would make the overall loss very small</li>
          <li>Assuming that there are N classes with S samples in each class, this method produces a total of NS(S−1)(NS−S) triplets. (NS anchors, S−1 possible positives per anchor, NS - S possible negatives)
</li>
        </ul></li>
        <li>
          Batch hard: For each anchor, select the hardest positive (largest distance amongst all d(a,p)) and the hardest negative (least distance amongst all d(a,n)) among the batch <dt-cite key="tripletdefense"></dt-cite>.  
          <ul>
            <li>Again assuming that there are N classes with S samples in each class, this method produces NS triplets</li>
            <li>The selected triplets are the hardest among the batch</li>
          </ul>

        </li>
    </ol></li>
</ol></p>
  <h2>Conclusion</h2>
  <p>
In this paper, we exlpain how one can use siamese networks for one-shot learning. We also explain the advantages of using triplet loss over contrastive loss. Further, we include visualizations that helps one understand how the value of margin and the distance metric affect the embedding learned using siamese networks with triplet loss. Finally, we also explain how one should select triplets and perform triplet mining for optimizing the training phase of siamese networks. 
 </p>   
 <p>
  Some other useful material can be found at  <dt-cite key="arsenault_2018"></dt-cite>,<dt-cite key="qian2019softtriple"></dt-cite>,<dt-cite key="DBLP:journals/corr/abs-1905-11713"></dt-cite>,<dt-cite key="gandhi_2018"></dt-cite>,<dt-cite key="DBLP:journals/corr/HermansBL17"></dt-cite>

  </p>
</dt-article>
</body>
<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @inproceedings{facenet,
  title={Facenet: A unified embedding for face recognition and clustering},
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={815--823},
  year={2015}
}

@article{tripletdefense,
  title={In defense of the triplet loss for person re-identification},
  author={Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
  journal={arXiv preprint arXiv:1703.07737},
  year={2017}
}

@inproceedings{contrastive,
  title={Dimensionality reduction by learning an invariant mapping},
  author={Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  volume={2},
  pages={1735--1742},
  year={2006},
  organization={IEEE}
}
@misc{arsenault_2018, title={Lossless Triplet loss}, url={https://towardsdatascience.com/lossless-triplet-loss-7e932f990b24}, journal={Medium}, publisher={Towards Data Science}, author={Arsenault, Marc-Olivier}, year={2018}, month={Feb}}

@misc{qian2019softtriple,
    title={SoftTriple Loss: Deep Metric Learning Without Triplet Sampling},
    author={Qi Qian and Lei Shang and Baigui Sun and Juhua Hu and Hao Li and Rong Jin},
    year={2019},
    eprint={1909.05235},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{DBLP:journals/corr/abs-1905-11713,
  author    = {Pengcheng Li and
               Jinfeng Yi and
               Bowen Zhou and
               Lijun Zhang},
  title     = {Improving the Robustness of Deep Neural Networks via Adversarial Training
               with Triplet Loss},
  journal   = {CoRR},
  volume    = {abs/1905.11713},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.11713},
  archivePrefix = {arXiv},
  eprint    = {1905.11713},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-11713},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gandhi_2018, title={Siamese Network & Triplet Loss}, url={https://towardsdatascience.com/siamese-network-triplet-loss-b4ca82c1aec8}, journal={Medium}, publisher={Towards Data Science}, author={Gandhi, Rohith}, year={2018}, month={May}}

@article{DBLP:journals/corr/HermansBL17,
  author    = {Alexander Hermans and
               Lucas Beyer and
               Bastian Leibe},
  title     = {In Defense of the Triplet Loss for Person Re-Identification},
  journal   = {CoRR},
  volume    = {abs/1703.07737},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.07737},
  archivePrefix = {arXiv},
  eprint    = {1703.07737},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HermansBL17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

</script>
